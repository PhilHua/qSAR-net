{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports - general\n",
    "import random\n",
    "random.seed(15)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "from math import ceil, sqrt\n",
    "#import sklearn\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "\n",
    "#Imports - RDKit\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.Chem.rdMolDescriptors import *\n",
    "\n",
    "#Imports - additional\n",
    "from os import listdir #for getting a list of files in a dir to process checkpoints\n",
    "import time # for real-time timing the progress of the network\n",
    "\n",
    "path = 'data/targets/'   #path to .ism files\n",
    "batch_number = 0\n",
    "#Get the list of filenames in data/targets/\n",
    "#ordered by the size of class, descending:\n",
    "with open(\"receptors_desc.txt\", \"r\") as f:\n",
    "    receptors = [l.strip().split() for l in f.readlines()]\n",
    "#receptors[:5]\n",
    "\n",
    "#---\n",
    "\n",
    "# Functions\n",
    "\n",
    "## Preparing the dataset\n",
    "\n",
    "def read_ism(file_name):\n",
    "    ''' Parse an .ism file, returning a list of smiles of molecules '''\n",
    "    mol_list = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        frl = f.readlines()\n",
    "    for line in frl:\n",
    "        line = line.split('\\t')\n",
    "        smile = line[0]\n",
    "        mol_list.append(smile)\n",
    "    return mol_list\n",
    "\n",
    "#### Creating molecule representations\n",
    "\n",
    "def get_class_vectors(mol_matrix, rType = 'fingerprint', f_size=2048):\n",
    "    ''' For each molecule in the array (list of lists):\n",
    "        create its vector representation for training (fingerprint/descriptor/mixed)\n",
    "        and a binary vector showing which classes it belongs to.\n",
    "        f_size - length of the fingerprint\n",
    "    '''\n",
    "    cv_dict = {}\n",
    "    for i,l in enumerate(mol_matrix):\n",
    "        for smile in l:\n",
    "            if smile not in cv_dict:\n",
    "                \n",
    "                mol = MolFromSmiles(smile)\n",
    "                if rType == 'fingerprint':\n",
    "                    rep = fingerprint(mol, f_size)\n",
    "                elif rType == 'descriptor':\n",
    "                    rep = descriptor(mol)\n",
    "                else: # both representations\n",
    "                    rep = descriptor(mol) + list(fingerprint(mol,f_size))\n",
    "                \n",
    "                labels = [0]*len(mol_matrix)\n",
    "                cv_dict[smile] = [rep, labels]\n",
    "            cv_dict[smile][1][i] = 1\n",
    "    # Normalize descriptors\n",
    "    if rType != 'fingerprint':\n",
    "        matrix_to_norm = []\n",
    "        for vals in cv_dict.values():\n",
    "            matrix_to_norm.append(vals[0][:37])\n",
    "        matrix_to_norm = np.array(matrix_to_norm)\n",
    "        v_min = matrix_to_norm.min(axis=0)\n",
    "        v_max = matrix_to_norm.max(axis=0) - v_min\n",
    "        \n",
    "        for key in cv_dict.keys():\n",
    "            for i in range(37):\n",
    "                normed_val = (cv_dict[key][0][i] - v_min[i]) / v_max[i]\n",
    "                cv_dict[key][0][i] = normed_val\n",
    "        \n",
    "    return cv_dict\n",
    "\n",
    "def fingerprint(mol, f_size=2048):\n",
    "    return GetMorganFingerprintAsBitVect(mol,2,f_size)\n",
    "\n",
    "def descriptor(mol):\n",
    "    functions = [CalcChi0n,\n",
    "            CalcChi0v,\n",
    "            CalcChi1n,\n",
    "            CalcChi1v,\n",
    "            CalcChi2n,\n",
    "            CalcChi2v,\n",
    "            CalcChi3n,\n",
    "            CalcChi3v,\n",
    "            CalcChi4n,\n",
    "            CalcChi4v,\n",
    "            CalcExactMolWt,\n",
    "            CalcFractionCSP3,\n",
    "            CalcHallKierAlpha,\n",
    "            CalcKappa1,\n",
    "            CalcKappa2,\n",
    "            CalcKappa3,\n",
    "            CalcLabuteASA,\n",
    "            CalcNumAliphaticCarbocycles,\n",
    "            CalcNumAliphaticHeterocycles,\n",
    "            CalcNumAliphaticRings,\n",
    "            CalcNumAmideBonds,\n",
    "            CalcNumAromaticCarbocycles,\n",
    "            CalcNumAromaticHeterocycles,\n",
    "            CalcNumAromaticRings,\n",
    "            CalcNumBridgeheadAtoms,\n",
    "            CalcNumHBA,\n",
    "            CalcNumHBD,\n",
    "            CalcNumHeteroatoms,\n",
    "            CalcNumHeterocycles,\n",
    "            CalcNumLipinskiHBA,\n",
    "            CalcNumLipinskiHBD,\n",
    "            CalcNumRings,\n",
    "            CalcNumSaturatedCarbocycles,\n",
    "            CalcNumSaturatedHeterocycles,\n",
    "            CalcNumSaturatedRings,\n",
    "            CalcNumSpiroAtoms,\n",
    "            CalcTPSA]\n",
    "        \n",
    "    descriptors = []\n",
    "    for function in functions:\n",
    "        descriptors.append(function(mol))\n",
    "    return descriptors\n",
    "\n",
    "def prepare_sets(mol_matrix, val_prc=0.25, rType='fingerprint', f_size=2048):\n",
    "    ''' Create training and validation sets with labels from an array of smiles\n",
    "        rType - type of representation - fingerprint/descriptor/mixed\n",
    "        f_size - size of the fingerprint if rType is fingerprint or mixed '''\n",
    "    # Create a representation and a class vector for each molecule\n",
    "    molecules = get_class_vectors(mol_matrix,rType,f_size)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    mts=[[],[]]\n",
    "    for val in molecules.values():\n",
    "        mts[0].append(val[0]) # [fingerprint]\n",
    "        mts[1].append(val[1]) # [label]\n",
    "\n",
    "    molecules_split = train_test_split(mts[0], mts[1], test_size=val_prc, random_state=15)\n",
    "\n",
    "    # Validation set:\n",
    "    #     a list of representations (mols) and\n",
    "    #     a list of corresponding class vectors (labels)\n",
    "    val_mols = np.array(molecules_split[1])\n",
    "    val_labels = np.array(molecules_split[3])\n",
    "    # Training set:\n",
    "    train_set = [[],[]]\n",
    "    train_set[0] = np.array(molecules_split[0])\n",
    "    train_set[1] = np.array(molecules_split[2])\n",
    "    \n",
    "    return train_set, val_mols, val_labels\n",
    "\n",
    "## Neuron layers\n",
    "\n",
    "def neuron_layer(isize, hsize, prev_layer):\n",
    "    ''' Create a single neuron layer - weight, bias, placeholder '''\n",
    "    # weights and biases\n",
    "    w = tf.Variable(tf.random_normal((isize, hsize), stddev=1/sqrt(isize)))\n",
    "    b = tf.Variable(tf.random_normal((hsize,), stddev=0.1))\n",
    "    # neuron\n",
    "    h = tf.nn.relu(tf.matmul(prev_layer, w) + b)\n",
    "    return w,b,h\n",
    "\n",
    "def setup_layers(layers, osize, isize=2048):\n",
    "    ''' Create placeholders, weights and biases for all requested layers '''\n",
    "    x = tf.placeholder(tf.float32, shape=[None,isize])\n",
    "    active_layers = {'x':x,'w':[],'b':[],'h':[]}\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        if i == 0:\n",
    "            w,b,h = neuron_layer(isize, layers[i], x)\n",
    "        else:\n",
    "            w,b,h = neuron_layer(layers[i-1], layers[i], prev_h)\n",
    "        active_layers['w'].append(w)\n",
    "        active_layers['b'].append(b)\n",
    "        active_layers['h'].append(h)\n",
    "        prev_h = h\n",
    "    \n",
    "    # Output Layer\n",
    "    wo = tf.Variable(tf.random_normal((layers[-1], osize), stddev=1/sqrt(layers[-1])))\n",
    "    bo = tf.Variable(tf.random_normal((osize,), stddev=0.1))\n",
    "    a = tf.matmul(h, wo) + bo #h is the one last initialized in the loop > of the last layer\n",
    "    \n",
    "    # Placeholder for targets\n",
    "    t = tf.placeholder(tf.float32, shape=[None, osize])\n",
    "    \n",
    "    active_layers['out'] = [wo,bo,a,t]\n",
    "    return active_layers\n",
    "\n",
    "## Training functions\n",
    "\n",
    "def next_batch(data, size): #data = [[fingerprints], [labels]]\n",
    "    ''' Extract the next batch from a dataset.\n",
    "        If dataset size is not dividable by batch size with reminder 0,\n",
    "       the last full batch and the remaining incomplete batch will be merged \n",
    "       to incorporate all data.'''\n",
    "    global batch_number\n",
    "    if batch_number == len(data[0])//size-1:\n",
    "        #the last batch may be larger to incorporate all data\n",
    "        # -1 because the first batch number is 0\n",
    "        start = batch_number*size\n",
    "        batch_number = 0\n",
    "        return data[0][start:], data[1][start:]\n",
    "    start = batch_number*size\n",
    "    batch_number += 1\n",
    "    return data[0][start:start+size], data[1][start:start+size]\n",
    "\n",
    "def shuffle_data(data):\n",
    "    '''Used to shuffle the training set after an epoch'''\n",
    "    indices = list(range(len(data[0])))\n",
    "    random.shuffle(indices)\n",
    "    new_data = [data[0][indices], data[1][indices]]\n",
    "    return new_data\n",
    "\n",
    "def timer(start,end):\n",
    "    ''' For timing the training of the network '''\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return \"{:0>2}:{:0>2}:{:0>2}\".format(int(hours),int(minutes),int(seconds))\n",
    "\n",
    "def tf_fractions(a1, a2):\n",
    "    ''' Calculate true/false positives/negatives in a2 based on a1\n",
    "        for each class seperately. '''\n",
    "    num_class = len(a1[0]) #number of classes\n",
    "    tp = np.array([0] * num_class)\n",
    "    fn = np.array([0] * num_class)\n",
    "    tn = np.array([0] * num_class)\n",
    "    fp = np.array([0] * num_class)\n",
    "    for i in range(num_class):\n",
    "        for j in range(len(a1)):\n",
    "            if a2[j][i] == a1[j][i]:\n",
    "                if a1[j][i] == 1:\n",
    "                    tp[i] += 1\n",
    "                else:\n",
    "                    tn[i] += 1\n",
    "            else:\n",
    "                if a1[j][i] == 1:\n",
    "                    fn[i] += 1\n",
    "                else:\n",
    "                    fp[i] += 1\n",
    "    return [tp,fn,tn,fp]\n",
    "\n",
    "def train_network(data, batch_size, epochs, verbose=True, save=True):\n",
    "    ''' This function trains the network, returning the rate of training and validation accuracy '''\n",
    "    # Variables:\n",
    "    global start, train, val_pred, batch_number\n",
    "    iterations = len(data[0]) // batch_size * epochs\n",
    "    if verbose: print(iterations, ' iterations')\n",
    "    tr_rate = []\n",
    "    val_rate = []\n",
    "    f1 = [[],[],[]]\n",
    "    tf_frac = []\n",
    "    batch_number = 0\n",
    "    save_step=0\n",
    "    # Training:\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "\n",
    "        if save:\n",
    "            saver.save(session, \"tmp/checkpoint\", global_step=save_step)\n",
    "        \n",
    "        # Train the network\n",
    "        for i in range(iterations):\n",
    "            if i != 0 and batch_number == 0:\n",
    "                data = shuffle_data(data)\n",
    "            \n",
    "            mols, labels = next_batch(data, batch_size)\n",
    "            \n",
    "            session.run(train, feed_dict={x: mols, t: labels})\n",
    "            \n",
    "            # Display progress and make a checkpoint\n",
    "            if i == 0 or (i+1)%(iterations//10) == 0:\n",
    "                if save:\n",
    "                    save_step += 1\n",
    "                    saver.save(session, \"tmp/checkpoint\", global_step = save_step)\n",
    "                if verbose:\n",
    "                    progress = int((i+1)/(iterations//10))*10\n",
    "                    now = timer(start,time.time())\n",
    "                    print(\"%3i\" % progress + '%, ' + str(now))                \n",
    "            \n",
    "            # Validate predictions\n",
    "            if i % 200 == 0 or i == iterations-1:\n",
    "                \n",
    "                tr_target = labels\n",
    "                tr_predictions = session.run(predict, feed_dict={x: mols, t: labels})\n",
    "                tr_rate.append((tr_target == tr_predictions).all(axis=1).mean())\n",
    "\n",
    "                val_predictions = session.run(predict, feed_dict={x: val_mols, t: val_labels})\n",
    "                val_rate.append((val_labels == val_predictions).all(axis=1).mean())\n",
    "                \n",
    "                \n",
    "                f1[0].append(f1_score(val_labels, val_predictions, average='micro'))\n",
    "                f1[1].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "                f1[2].append(f1_score(val_labels, val_predictions, average='weighted'))\n",
    "                \n",
    "        tf_frac = tf_fractions(val_labels,val_predictions)\n",
    "        # Final checkpoint\n",
    "        if save:\n",
    "            saver.save(session, \"tmp/checkpoint\", global_step=save_step+1)\n",
    "    return tr_rate, val_rate, f1, tf_frac\n",
    "\n",
    "def filename_gen(plot_type='', path=''):\n",
    "    ''' Generate filename for a plot. '''\n",
    "    if plot_type != '':\n",
    "        plot_type += '_'\n",
    "    filename = plot_type\n",
    "    layers_str = \"{}'{}\".format(len(layers),max(layers))\n",
    "    filename += \"{}_{}_{}_{:>6}_{}_{}_{:.4}_{}\".format(\n",
    "                num_class, f_size, layers_str, learning_rate, batch_size, epochs, max(val_rate), rType)\n",
    "    filename = filename.replace(\".\",\"'\")\n",
    "    filename = filename.replace(\" \",\"_\")\n",
    "    filename = path + filename + \".pdf\"\n",
    "    return filename\n",
    "\n",
    "def save_accuracy_plot(tr_rate, save=True, path=\"plots/\"):\n",
    "    ''' Draws and saves a plot which displays changes of accuracy \n",
    "        during training for training and validation sets. '''\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    x = [i*200 for i in range(len(tr_rate))]\n",
    "    ax.plot(x,tr_rate, label='trening')\n",
    "    ax.plot(x,val_rate, label='walidacja')\n",
    "    ax.legend(loc=5)\n",
    "    ax.set_ylabel('Dokładność')\n",
    "    ax.set_xlabel('Krok')\n",
    "    plt.ylim([0,1])\n",
    "    vals = ax.get_yticks()\n",
    "    ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    \n",
    "    title = \"Cele terapeutyczne: {:2}, Szybkość uczenia: {}\\nWarstwy: {}, Rozmiar batcha: {}, Liczba epok: {}\".format(\n",
    "        num_class, learning_rate, layers, batch_size, epochs)\n",
    "    plt.title(title);\n",
    "    \n",
    "    if save:\n",
    "        filename = filename_gen('',path)\n",
    "        plt.savefig(filename);\n",
    "\n",
    "def save_f1_plot(f1, save=True, path=\"plots/\"):\n",
    "    ''' Draws and saves a plot which displays changes of F1-score\n",
    "        during training for the validation set.\n",
    "        F1 calculated and drawn in three types of averaging. '''\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    x = [i*200 for i in range(len(f1[0]))]\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(x,f1[0], label='F1 micro')\n",
    "    ax.plot(x,f1[1], label='F1 macro')\n",
    "    ax.plot(x,f1[2], label='F1 ważone')\n",
    "    ax.legend(loc=7)\n",
    "    ax.set_ylabel('F1')\n",
    "    ax.set_xlabel('Krok')\n",
    "    plt.ylim([0,1])\n",
    "    vals = ax.get_yticks()\n",
    "    ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    \n",
    "    title = \"Cele terapeutyczne: {:2}, Szybkość uczenia: {}\\nWarstwy: {}, Rozmiar batcha: {}, Liczba epok: {}\".format(\n",
    "        num_class, learning_rate, layers, batch_size, epochs)\n",
    "    plt.title(title);\n",
    "    \n",
    "    if save:\n",
    "        filename = filename_gen('f1',path)\n",
    "        plt.savefig(filename);\n",
    "\n",
    "def save_tf_fractions_plot(tff, save=True, path='plots/'):\n",
    "    ''' Draws and saves a histogram of true/false positive/negative fractions:\n",
    "        True Positive rate, False Positive Rate, False Negative Rate and\n",
    "        False Discovery Rate.\n",
    "        True Negative Rate not included due to having values indistinguishably\n",
    "        close to 100%; however, TNR = 100% - FPR, and the FPR histogram is more\n",
    "        visually clear. '''\n",
    "    plt.close()\n",
    "    fig = plt.figure()\n",
    "    subplots = [fig.add_subplot(221), fig.add_subplot(222),\n",
    "                fig.add_subplot(223), fig.add_subplot(224)]\n",
    "    titles = ['Odsetek prawdziwie pozytywnych', 'Odsetek fałszywie pozytywnych',\n",
    "              'Odsetek fałszywie negatywnych', 'Odsetek fałszywych odkryć']\n",
    "    rate_values = [tff[0]/(tff[0]+tff[1]), tff[3]/(tff[2]+tff[3]),\n",
    "                   tff[1]/(tff[0]+tff[1]), tff[3]/(tff[0]+tff[3])]\n",
    "    xlabel = 'Cele terapeutyczne'\n",
    "    xrange = range(1,len(tff[0])+1)\n",
    "    \n",
    "    for i, ax in enumerate(subplots):\n",
    "        ax.bar(xrange, rate_values[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xlim([0,num_class+2])\n",
    "        vals = ax.get_yticks()\n",
    "        if i == 1: #false positive rate is very small\n",
    "            ax.set_yticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "        else:\n",
    "            ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    title = \"Cele terapeutyczne: {:2}, Szybkość uczenia: {}\\nWarstwy: {}, Rozmiar batcha: {}, Liczba epok: {}\".format(\n",
    "        num_class, learning_rate, layers, batch_size, epochs)\n",
    "    fig.suptitle(title)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    \n",
    "    if save:\n",
    "        filename = filename_gen('tff',path)\n",
    "        plt.savefig(filename)\n",
    "        \n",
    "import pickle\n",
    "\n",
    "def load_var(name):\n",
    "    with open(name,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_var(name,var):\n",
    "    with open(name,'wb') as f:\n",
    "        pickle.dump(var,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create new examples:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training with saving val_pred_roc to a global variable\n",
    "val_pred_roc = 0\n",
    "\n",
    "def train_network(data, batch_size, epochs, verbose=True, save=True):\n",
    "    ''' This function trains the network, returning the rate of training and validation accuracy '''\n",
    "    # Variables:\n",
    "    global start, train, val_pred_roc, batch_number\n",
    "    iterations = len(data[0]) // batch_size * epochs\n",
    "    if verbose: print(iterations, ' iterations')\n",
    "    tr_rate = []\n",
    "    val_rate = []\n",
    "    f1 = [[],[],[]]\n",
    "    tf_frac = []\n",
    "    batch_number = 0\n",
    "    save_step=0\n",
    "    # Training:\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "\n",
    "        if save:\n",
    "            saver.save(session, \"tmp/checkpoint\", global_step=save_step)\n",
    "        \n",
    "        # Train the network\n",
    "        for i in range(iterations):\n",
    "            if i != 0 and batch_number == 0:\n",
    "                data = shuffle_data(data)\n",
    "            \n",
    "            mols, labels = next_batch(data, batch_size)\n",
    "            \n",
    "            session.run(train, feed_dict={x: mols, t: labels})\n",
    "            \n",
    "            # Display progress and make a checkpoint\n",
    "            if i == 0 or (i+1)%(iterations//10) == 0:\n",
    "                if save:\n",
    "                    save_step += 1\n",
    "                    saver.save(session, \"tmp/checkpoint\", global_step = save_step)\n",
    "                if verbose:\n",
    "                    progress = int((i+1)/(iterations//10))*10\n",
    "                    now = timer(start,time.time())\n",
    "                    print(\"%3i\" % progress + '%, ' + str(now))                \n",
    "            \n",
    "            # Validate predictions\n",
    "            if i % 200 == 0 or i == iterations-1:\n",
    "                \n",
    "                tr_target = labels\n",
    "                tr_predictions = session.run(predict, feed_dict={x: mols, t: labels})\n",
    "                tr_rate.append((tr_target == tr_predictions).all(axis=1).mean())\n",
    "\n",
    "                val_pred_roc = session.run(y, feed_dict={x: val_mols, t: val_labels})\n",
    "                \n",
    "                val_predictions = session.run(predict, feed_dict={x: val_mols, t: val_labels})\n",
    "                val_rate.append((val_labels == val_predictions).all(axis=1).mean())\n",
    "                \n",
    "                \n",
    "                f1[0].append(f1_score(val_labels, val_predictions, average='micro'))\n",
    "                f1[1].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "                f1[2].append(f1_score(val_labels, val_predictions, average='weighted'))\n",
    "                \n",
    "        tf_frac = tf_fractions(val_labels,val_predictions)\n",
    "        # Final checkpoint\n",
    "        if save:\n",
    "            saver.save(session, \"tmp/checkpoint\", global_step=save_step+1)\n",
    "    return tr_rate, val_rate, f1, tf_frac\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "num_class = 73 #first n most numerous classes\n",
    "rType = 'fingerprint' # fingerprint/descriptor/mixed\n",
    "f_size = 4096\n",
    "\n",
    "mol_matrix = []\n",
    "for line in receptors[:num_class]:\n",
    "    name = line[0]\n",
    "    mol_list = read_ism(path + name)\n",
    "    mol_matrix.append(mol_list)\n",
    "mol_matrix[0][:3]\n",
    "\n",
    "random.seed(15)\n",
    "np.random.seed(15)\n",
    "val_pred_roc = 0\n",
    "train_set,val_mols,val_labels = prepare_sets(mol_matrix, 0.2, rType, f_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layers = [800,800]\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(15)\n",
    "np.random.seed(15)\n",
    "random.seed(15)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "start = time.time()\n",
    "# Automatic network variables\n",
    "isize = len(val_mols[0])\n",
    "osize = num_class\n",
    "active_layers = setup_layers(layers, osize, isize)\n",
    "x = active_layers['x'] # first placeholder\n",
    "a = active_layers['out'][2] # output layer activations\n",
    "t = active_layers['out'][3] # placeholder for targets\n",
    "saver = tf.train.Saver(max_to_keep=12)\n",
    "# Objective\n",
    "'''Adam Optimizer'''\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(a, t))\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "# Prediction\n",
    "y = tf.nn.sigmoid(a)\n",
    "predict = tf.round(y)\n",
    "# Train the network\n",
    "tr_rate, val_rate, f1, tf_frac = train_network(train_set, batch_size, epochs, save=False)\n",
    "full_time = timer(start,time.time())\n",
    "print(full_time)\n",
    "\"--END--\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading variables saved in files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "np.set_printoptions(precision=3)#default precision=8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is how they look when saving:\n",
    "\n",
    "# First iteration\n",
    "dat1 = {'tff':tf_frac,     #[true positive, false negative, true negative, false positive]\n",
    "        'vpr':val_pred_roc #last prediction values for the validation set\n",
    "       }\n",
    "save_var('random_seed/dat1.txt', dat1)\n",
    "# Second iteration\n",
    "dat2 = {'tff':tf_frac,\n",
    "        'vpr':val_pred_roc\n",
    "       }\n",
    "save_var('random_seed/dat2.txt', dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat1 = load_var('random_seed/dat1.txt')\n",
    "dat2 = load_var('random_seed/dat2.txt')\n",
    "\n",
    "tff1 = dat1['tff']\n",
    "tff2 = dat2['tff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([34, 50, 70, 72]),) [  6.48854962   5.11363636   7.14285714  10.        ]\n"
     ]
    }
   ],
   "source": [
    "def tp(tff):\n",
    "    return (tff[0]/(tff[0]+tff[1]))*100\n",
    "\n",
    "tp_diff = abs(tp(tff1)-tp(tff2))\n",
    "where = np.where(tp_diff>=5) #=[34,50,70,72]\n",
    "print(where,tp_diff[where])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vpr1 = np.transpose(dat1['vpr'])[where]\n",
    "vpr2 = np.transpose(dat2['vpr'])[where]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45830932  0.46535069  0.55543202  0.56893355  0.44075245  0.52805096\n",
      "  0.54730171  0.57909626  0.40319604  0.45612869  0.43193114  0.50551683\n",
      "  0.44401535  0.45406419  0.4334023   0.56521147  0.50617313  0.45471871] \n",
      " [ 0.01807402  0.02670146  0.89881915  0.20726749  0.03037917  0.3725228\n",
      "  0.0556124   0.00991256  0.13183682  0.14797866  0.00358292  0.02836937\n",
      "  0.53550476  0.25896418  0.10917896  0.01448439  0.01880406  0.46757531] \n",
      "\n",
      " [ 0.83434689  0.8886258   0.79487962  0.78034812  0.75695109  0.17841274\n",
      "  0.93325675  0.86481446  0.44401535  0.75695109  0.99957603  0.45471871] \n",
      " [ 0.47251084  0.59315091  0.40713882  0.44415307  0.58417165  0.46368834\n",
      "  0.43259549  0.44519094  0.53550476  0.58417165  0.56925076  0.46757531] \n",
      "---\n",
      "[ 0.42132023  0.42705861  0.45029482  0.59375715  0.5582127   0.56526804\n",
      "  0.59111178  0.46548209  0.5431118   0.5636999   0.45649257  0.52217704\n",
      "  0.46018896  0.46820307  0.568326  ] \n",
      " [ 0.24361387  0.05599518  0.04614478  0.03191292  0.70169497  0.3430396\n",
      "  0.09399357  0.04581396  0.06210491  0.52802461  0.73934215  0.10727573\n",
      "  0.72212011  0.17271729  0.46326849] \n",
      "\n",
      " [ 0.73505098  0.69340587  0.74151146  0.99059522  0.94695121  0.94516224\n",
      "  0.11234174  0.5636999   0.89366394  0.17696853  0.6525141   0.98508912\n",
      "  0.568326    0.84604019  0.33713734  0.82114583] \n",
      " [ 0.43529636  0.50410539  0.5765273   0.45669731  0.43505532  0.49404562\n",
      "  0.41836923  0.52802461  0.59267241  0.52825284  0.59747732  0.59556597\n",
      "  0.46326849  0.43775949  0.49647567  0.58602715] \n",
      "---\n",
      "[] \n",
      " [] \n",
      "\n",
      " [] \n",
      " [] \n",
      "---\n",
      "[] \n",
      " [] \n",
      "\n",
      " [] \n",
      " [] \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    w1 = np.where((vpr1[i]>.4)&(vpr1[i]<.6))\n",
    "    w2 = np.where((vpr2[i]>.4)&(vpr2[i]<.6))\n",
    "    print(vpr1[i][w1],'\\n',vpr2[i][w1],'\\n\\n',vpr1[i][w2],'\\n',vpr2[i][w2],'\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes 70 and 72 (numbered from 0) don't have predictions within the interval (0.4, 0.6)!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
